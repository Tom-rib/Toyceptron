# ğŸ§  Toyceptron

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![Status](https://img.shields.io/badge/Status-ComplÃ©tÃ©-green.svg)]()
[![License](https://img.shields.io/badge/License-Educational-green.svg)]()

> *"What I cannot create, I do not understand"* â€” Richard Feynman

Un **vrai rÃ©seau de neurones** codÃ© from scratch, sans bibliothÃ¨ques externes (numpy, pytorch, sklearn...).  
Projet pÃ©dagogique pour comprendre la structure d'un perceptron multi-couches.

---

## ğŸ“‹ Sommaire du projet

### ğŸ“š Cours & Ressources

Tous les fichiers pÃ©dagogiques sont dans le dossier `cours/` :

| # | Fichier | Contenu |
|---|---------|---------|
| 01 | [Introduction](cours/01_introduction.md) | Contexte, objectifs, philosophie du projet |
| 02 | [Concepts thÃ©oriques](cours/02_concepts_theoriques.md) | Neurone, couche, rÃ©seau, fonctions d'activation |
| 03 | [PrÃ©paration](cours/03_preparation.md) | PrÃ©requis, setup, structure du projet |
| 04 | [ImplÃ©mentation Neuron](cours/04_implementation_neuron.md) | Coder la classe `Neuron` |
| 05 | [ImplÃ©mentation Layer](cours/05_implementation_layer.md) | Coder la classe `Layer` |
| 06 | [ImplÃ©mentation Network](cours/06_implementation_network.md) | Coder la classe `Network` |
| 07 | [Tests & Validation](cours/07_tests_validation.md) | VÃ©rifications et tests |
| 08 | [Bonus](cours/08_bonus.md) | FonctionnalitÃ©s avancÃ©es (optionnel) |
| 09 | [Troubleshooting](cours/09_troubleshooting.md) | RÃ©solution de problÃ¨mes |
| 10 | [Annexes](cours/10_annexes.md) | Formules, commandes Git, ressources |

### ğŸ’» Code source

| Fichier | RÃ´le |
|---------|------|
| `src/neuron.py` | Classe `Neuron` â€” UnitÃ© de calcul Ã©lÃ©mentaire |
| `src/layer.py` | Classe `Layer` â€” Collection de neurones |
| `src/network.py` | Classe `Network` â€” Composition de couches |
| `src/activation.py` | Fonctions d'activation (identity, threshold, relu) |
| `src/main.py` | Script de test fourni par le sujet |

---

## ğŸ¯ Objectifs

CrÃ©er un perceptron multi-couches capable de :
- âœ… Stocker des poids et biais dans chaque neurone
- âœ… Effectuer une **forward pass** complÃ¨te
- âœ… Utiliser 4 fonctions d'activation : identitÃ©, seuil, sigmoÃ¯de, ReLU
- âœ… Respecter l'interface imposÃ©e par le `main.py` fourni

**Contrainte :** Python pur uniquement â€” aucune bibliothÃ¨que externe !

---

## ğŸš€ Quick Start

### PrÃ©requis
```bash
# VÃ©rifier Python (version 3.8+)
python --version
```

### Installation
```bash
# Cloner le projet
git clone https://github.com/ton-username/toyceptron.git
cd toyceptron
```

### Lancer le projet
```bash
cd src
python main.py
```

### RÃ©sultat attendu
```
Input: [1.0, 2.0, 4.0]

--- Test Neuron ---
Neurone h1 (brut): 1.6
Neurone h2 (brut): 0.7
Neurone h1 (activÃ©): 0.8320183851339245
Neurone h2 (activÃ©): 0.6681877721681662

--- Test Layer ---
Couche (valeurs brutes): [1.6, 0.7]
Couche (valeurs activÃ©es): [0.8320183851339245, 0.6681877721681662]

--- Test Network ---
Sorties activÃ©es : [0.5309442148001715, 0.494901997674804]
```

---

## ğŸ—‚ï¸ Structure du dÃ©pÃ´t

```
toyceptron/
â”œâ”€â”€ README.md                       # Ce fichier
â”œâ”€â”€ .gitignore                      # Fichiers ignorÃ©s par Git
â”‚
â”œâ”€â”€ cours/                          # ğŸ“š Documentation pÃ©dagogique
â”‚   â”œâ”€â”€ 01_introduction.md
â”‚   â”œâ”€â”€ 02_concepts_theoriques.md
â”‚   â”œâ”€â”€ 03_preparation.md
â”‚   â”œâ”€â”€ 04_implementation_neuron.md
â”‚   â”œâ”€â”€ 05_implementation_layer.md
â”‚   â”œâ”€â”€ 06_implementation_network.md
â”‚   â”œâ”€â”€ 07_tests_validation.md
â”‚   â”œâ”€â”€ 08_bonus.md
â”‚   â”œâ”€â”€ 09_troubleshooting.md
â”‚   â””â”€â”€ 10_annexes.md
â”‚
â””â”€â”€ src/                            # ğŸ’» Code source
    â”œâ”€â”€ neuron.py                   # Classe Neuron
    â”œâ”€â”€ layer.py                    # Classe Layer
    â”œâ”€â”€ network.py                  # Classe Network
    â”œâ”€â”€ activation.py               # Fonctions d'activation
    â””â”€â”€ main.py                     # Script de test (fourni)
```

---

## ğŸ§© Architecture du code

### SÃ©paration des responsabilitÃ©s

```
neuron.py         layer.py          network.py        activation.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Neuron            Layer             Network           act_identity()
  â”œâ”€â”€ weights       â”œâ”€â”€ neurons[]     â”œâ”€â”€ layers[]      act_threshold()
  â”œâ”€â”€ bias          â””â”€â”€ forward()     â”œâ”€â”€ activation    act_relu()
  â””â”€â”€ forward()     (valeurs brutes)  â”œâ”€â”€ add()
  (valeurs brutes)                    â””â”€â”€ feedforward()
                                      (applique activation)
```

### Principe clÃ© : sÃ©paration brut / activÃ©

```
Neuron.forward()   â†’  valeur BRUTE  z = Î£(wiÂ·xi) + b
Layer.forward()    â†’  valeurs BRUTES [z1, z2, ...]
Network.feedforward() â†’  valeurs ACTIVÃ‰ES [f(z1), f(z2), ...]
```

L'activation est appliquÃ©e **uniquement** dans le Network, pas dans les neurones ou les couches.

---

## ğŸ’» Exemple d'utilisation

### Neurone individuel
```python
from neuron import Neuron

neuron = Neuron(weights=[0.2, -0.1, 0.4], bias=0.0)
z = neuron.forward([1.0, 2.0, 4.0])
# z = 0.2*1.0 + (-0.1)*2.0 + 0.4*4.0 + 0.0 = 1.6
```

### Couche de neurones
```python
from layer import Layer

layer = Layer(
    weights_list=[[0.2, -0.1, 0.4], [-0.4, 0.3, 0.1]],
    biases_list=[0.0, 0.1]
)
raw = layer.forward([1.0, 2.0, 4.0])
# raw = [1.6, 0.7]  (valeurs brutes)
```

### RÃ©seau complet
```python
from network import Network
from math import exp

def act_sigmoid(x):
    return 1 / (1 + exp(-x))

net = Network(input_size=3, activation=act_sigmoid)

net.add(
    weights=[[0.2, -0.1, 0.4], [-0.4, 0.3, 0.1]],
    biases=[0.0, 0.1]
)
net.add(
    weights=[[0.5, -0.2], [-0.3, 0.4], [0.1, 0.2]],
    biases=[0.0, 0.1, -0.1]
)
net.add(
    weights=[[0.3, -0.1, 0.2], [-0.5, 0.4, 0.1]],
    biases=[-0.1, 0.0]
)

y = net.feedforward([1.0, 2.0, 4.0])
# y = [0.5309442148001715, 0.494901997674804]
```

### Fonctions d'activation disponibles
```python
from activation import act_identity, act_threshold, act_relu

act_identity(x)   # f(x) = x
act_threshold(x)  # f(x) = 1 si x >= 0, sinon 0
act_relu(x)       # f(x) = max(0, x)
# act_sigmoid est dÃ©finie dans main.py
```

---

## ğŸ“ CompÃ©tences dÃ©veloppÃ©es

- **Python** : Programmation OrientÃ©e Objet (classes, mÃ©thodes, attributs)
- **Structures de donnÃ©es** : listes, manipulation de vecteurs
- **Machine Learning** : architecture MLP, forward pass, fonctions d'activation
- **Documentation** : Markdown, GitHub, docstrings

---

## ğŸ“š Ressources externes

- [VidÃ©o 3Blue1Brown - Neural Networks](https://www.youtube.com/watch?v=aircAruvnKk) (EN, 19min)
- [Article WikipÃ©dia - Perceptron](https://fr.wikipedia.org/wiki/Perceptron)
- [Playground TensorFlow](https://playground.tensorflow.org/) (visualiser un rÃ©seau)
- [W3Schools - Machine Learning](https://www.w3schools.com/python/python_ml_getting_started.asp)

---

## ğŸ‘¨â€ğŸ’» Auteur

**[Tom Ribero]** â€” Ã‰tudiant en Administration SystÃ¨mes et RÃ©seaux (2e annÃ©e)  
Projet rÃ©alisÃ© dans le cadre de [nom de l'Ã©cole/formation]
**[Romain Jazzar]** â€” Ã‰tudiant en  data IA (2e annÃ©e)  

---
