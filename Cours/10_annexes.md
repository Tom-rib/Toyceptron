# 10 - Annexes

> Formules math√©matiques, commandes utiles et ressources

---

## üìê Formules math√©matiques

### Produit scalaire (dot product)

**Formule :**
```
dot(x, w) = Œ£(xi ¬∑ wi) = x1¬∑w1 + x2¬∑w2 + ... + xn¬∑wn
```

**Exemple :**
```
x = [1, 2, 3]
w = [0.5, -0.2, 0.1]

dot = 1√ó0.5 + 2√ó(-0.2) + 3√ó0.1
    = 0.5 - 0.4 + 0.3
    = 0.4
```

**En Python :**
```python
def dot_product(x, w):
    return sum(xi * wi for xi, wi in zip(x, w))
```

---

### Sortie d'un neurone

**Formule :**
```
y = f(Œ£(wi ¬∑ xi) + b)
```

O√π :
- `xi` = entr√©es
- `wi` = poids
- `b` = biais
- `f` = fonction d'activation

**D√©compos√© :**
```
1. z = w1¬∑x1 + w2¬∑x2 + ... + wn¬∑xn + b
2. y = f(z)
```

---

### Fonctions d'activation

#### 1. Identit√©
```
f(x) = x
f'(x) = 1
```

#### 2. Seuil (Step)
```
f(x) = { 1 si x ‚â• 0
       { 0 si x < 0
```

#### 3. Sigmo√Øde
```
f(x) = 1 / (1 + e^(-x))
f'(x) = f(x) ¬∑ (1 - f(x))
```

**Propri√©t√©s :**
- Sortie entre 0 et 1
- `f(0) = 0.5`
- `f(‚àû) = 1`
- `f(-‚àû) = 0`

#### 4. ReLU (Rectified Linear Unit)
```
f(x) = max(0, x) = { x si x ‚â• 0
                    { 0 si x < 0
f'(x) = { 1 si x > 0
        { 0 si x ‚â§ 0
```

---

## üíª Commandes Python utiles

### Gestion de listes

```python
# Cr√©er une liste de N √©l√©ments
liste = [0] * 5  # [0, 0, 0, 0, 0]

# Liste avec range
liste = list(range(5))  # [0, 1, 2, 3, 4]

# Compr√©hension de liste
carres = [x**2 for x in range(5)]  # [0, 1, 4, 9, 16]

# Somme d'une liste
total = sum([1, 2, 3, 4])  # 10

# Longueur d'une liste
taille = len([1, 2, 3])  # 3

# Zip deux listes
for x, w in zip([1, 2], [0.5, 0.3]):
    print(x * w)
```

---

### Module random

```python
import random

# Nombre al√©atoire entre 0 et 1
r = random.random()

# Nombre al√©atoire entre a et b
r = random.uniform(-1, 1)

# Fixer la seed (r√©sultats reproductibles)
random.seed(42)

# Liste de N nombres al√©atoires
liste = [random.uniform(-1, 1) for _ in range(5)]
```

---

### Module math

```python
import math

# Exponentielle
e_x = math.exp(2)  # e^2

# Maximum
max_val = max(0, -5)  # 0

# Valeur absolue
abs_val = abs(-3)  # 3

# Puissance
pow_val = math.pow(2, 3)  # 8
```

---

## üîß Commandes Git

### Initialiser un d√©p√¥t

```bash
# Cr√©er un nouveau d√©p√¥t
git init

# V√©rifier le statut
git status

# Ajouter tous les fichiers
git add .

# Faire un commit
git commit -m "Initial commit"
```

---

### Pousser sur GitHub

```bash
# Lier au d√©p√¥t distant
git remote add origin https://github.com/username/toyceptron.git

# Pousser
git push -u origin main

# V√©rifier les remotes
git remote -v
```

---

### Workflow typique

```bash
# 1. Modifier des fichiers
# 2. Voir ce qui a chang√©
git status

# 3. Ajouter les changements
git add .

# 4. Commit avec message
git commit -m "Impl√©mentation de la classe Neuron"

# 5. Pousser sur GitHub
git push
```

---

## üìö Ressources externes

### Vid√©os

**3Blue1Brown - Neural Networks (EN, sous-titres FR)**
1. [But what is a neural network?](https://www.youtube.com/watch?v=aircAruvnKk) (19min)
2. [Gradient descent](https://www.youtube.com/watch?v=IHZwWFHWa-w) (21min)
3. [Backpropagation](https://www.youtube.com/watch?v=Ilg3gGewQ5U) (14min)

**En fran√ßais**
- [Machine Learnia - R√©seaux de neurones](https://www.youtube.com/watch?v=09e8-A5xkQE)
- [Science4All - Comment marchent les IA](https://www.youtube.com/watch?v=5EV9qPNfhWI)

---

### Articles et docs

**Wikip√©dia**
- [Perceptron](https://fr.wikipedia.org/wiki/Perceptron)
- [R√©seau de neurones artificiels](https://fr.wikipedia.org/wiki/R%C3%A9seau_de_neurones_artificiels)
- [Fonction d'activation](https://fr.wikipedia.org/wiki/Fonction_d%27activation)

**Autres ressources**
- [Neural Networks from Scratch](https://nnfs.io/) (EN)
- [Playground TensorFlow](https://playground.tensorflow.org/) (visualiser)
- [Distill.pub](https://distill.pub/) (articles visuels)

---

### Livres

**Gratuits**
- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) (EN, online)
- [Deep Learning](https://www.deeplearningbook.org/) (EN, online)

**Payants**
- "Make Your Own Neural Network" - Tariq Rashid
- "Neural Networks from Scratch in Python" - Harrison Kinsley

---

## üóÇÔ∏è Templates de fichiers

### .gitignore pour Python

```gitignore
# Python
__pycache__/
*.py[cod]
*$py.class
*.so

# Virtual environments
venv/
env/
ENV/

# IDEs
.vscode/
.idea/
*.swp
*.swo
.DS_Store

# Tests
.pytest_cache/
*.log

# Autres
*.egg-info/
dist/
build/
```

---

### README.md template

```markdown
# Toyceptron

R√©seau de neurones from scratch en Python pur.

## Installation

```bash
git clone https://github.com/username/toyceptron.git
cd toyceptron/src
python main.py
```

## Architecture

- `neuron.py` : Classe Neuron
- `layer.py` : Classe Layer
- `network.py` : Classe Network

## Usage

```python
from network import Network

network = Network([2, 5, 3, 1], activation='relu')
output = network.forward([1.0, 0.5])
print(output)
```

## Auteur

[Ton nom] - [ton email]
```

---

## üßÆ Exemples de calculs

### Exemple 1 : Neurone simple

**Donn√©es :**
- Poids : `[0.5, -0.3, 0.2]`
- Biais : `0.1`
- Entr√©es : `[1, 2, 3]`
- Activation : identit√©

**Calcul :**
```
z = 0.5√ó1 + (-0.3)√ó2 + 0.2√ó3 + 0.1
  = 0.5 - 0.6 + 0.6 + 0.1
  = 0.6

y = identity(0.6) = 0.6
```

---

### Exemple 2 : Couche avec 2 neurones

**Couche :**
- Neurone 1 : poids `[1, 1]`, biais `0`
- Neurone 2 : poids `[2, -1]`, biais `0.5`

**Entr√©e :** `[2, 3]`

**Calculs :**
```
Neurone 1:
  z1 = 1√ó2 + 1√ó3 + 0 = 5
  y1 = ReLU(5) = 5

Neurone 2:
  z2 = 2√ó2 + (-1)√ó3 + 0.5 = 1.5
  y2 = ReLU(1.5) = 1.5

Sortie : [5, 1.5]
```

---

### Exemple 3 : R√©seau complet [2‚Üí3‚Üí1]

**Architecture :** 2 entr√©es, 3 neurones cach√©s, 1 sortie

**Simplification (poids = 1, biais = 0, activation = identit√©) :**

```
Entr√©e : [1, 2]

Couche 1 (3 neurones) :
  Neurone 1 : z = 1√ó1 + 1√ó2 = 3 ‚Üí y = 3
  Neurone 2 : z = 1√ó1 + 1√ó2 = 3 ‚Üí y = 3
  Neurone 3 : z = 1√ó1 + 1√ó2 = 3 ‚Üí y = 3
  Sortie couche 1 : [3, 3, 3]

Couche 2 (1 neurone) :
  Neurone 1 : z = 1√ó3 + 1√ó3 + 1√ó3 = 9 ‚Üí y = 9

Sortie finale : 9
```

---

## üìù Glossaire

**Neurone (Neuron)**
: Unit√© de calcul √©l√©mentaire qui effectue une combinaison lin√©aire de ses entr√©es suivie d'une activation.

**Couche (Layer)**
: Ensemble de neurones qui re√ßoivent les m√™mes entr√©es et produisent chacun une sortie.

**R√©seau (Network)**
: Composition de plusieurs couches permettant de transformer des donn√©es complexes.

**Poids (Weights)**
: Param√®tres multiplicatifs d'un neurone. D√©termine l'importance de chaque entr√©e.

**Biais (Bias)**
: Param√®tre additif d'un neurone. Permet de d√©caler la fonction d'activation.

**Activation (Activation)**
: Fonction non-lin√©aire appliqu√©e √† la sortie d'un neurone (ReLU, sigmoid, etc.).

**Forward Pass**
: Propagation des donn√©es de l'entr√©e vers la sortie √† travers toutes les couches.

**Backpropagation**
: Algorithme d'apprentissage qui ajuste les poids pour minimiser l'erreur (pas dans ce projet).

**Perceptron**
: Mod√®le de r√©seau de neurones simple, souvent √† une seule couche.

**MLP (Multi-Layer Perceptron)**
: Perceptron avec plusieurs couches cach√©es.

---

## üéì Pour aller plus loin

### Concepts avanc√©s

1. **Entra√Ænement** : Ajuster automatiquement les poids pour minimiser une erreur
2. **Backpropagation** : Algorithme pour calculer les gradients
3. **Gradient descent** : M√©thode d'optimisation des poids
4. **Loss function** : Fonction mesurant l'erreur du r√©seau
5. **Overfitting / Underfitting** : Probl√®mes de g√©n√©ralisation

### Types de r√©seaux

- **CNN (Convolutional Neural Networks)** : Pour les images
- **RNN (Recurrent Neural Networks)** : Pour les s√©quences
- **Transformer** : Architecture moderne (GPT, BERT)
- **GAN (Generative Adversarial Networks)** : Pour g√©n√©rer des donn√©es

---

## üí° Conseils pour la suite

1. **Comprend avant d'utiliser des libs**
   - Maintenant que tu sais comment √ßa marche, utiliser PyTorch/TensorFlow sera plus clair

2. **Pratique r√©guli√®rement**
   - Code d'autres projets ML
   - Participe √† des comp√©titions Kaggle

3. **Reste curieux**
   - Lis des papers
   - Regarde des vid√©os
   - Exp√©rimente !

---

## üìû Contact et feedback

**Projet Toyceptron**
- GitHub : [github.com/ton-username/toyceptron](https://github.com)
- Email : ton-email@example.com

**La Plateforme**
- Site : [laplateforme.io](https://laplateforme.io)

---

## üèÜ Conclusion

**F√©licitations !** üéâ

Tu as maintenant une **compr√©hension profonde** des r√©seaux de neurones.

Tu sais :
- ‚úÖ Comment un neurone effectue un calcul
- ‚úÖ Comment une couche transforme des donn√©es
- ‚úÖ Comment un r√©seau propage l'information
- ‚úÖ Comment les activations influencent la sortie

**Cette connaissance est pr√©cieuse** et te servira pour :
- Utiliser des frameworks (PyTorch, TensorFlow)
- Comprendre les architectures avanc√©es
- D√©bugger des mod√®les de ML
- Expliquer le deep learning √† d'autres

---

**Bon courage pour la suite ! üöÄüß†**
