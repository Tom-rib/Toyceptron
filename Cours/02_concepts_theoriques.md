# 02 - Concepts th√©oriques

> Comprendre les bases math√©matiques avant de coder

---

## üß© Les 3 briques fondamentales

Un r√©seau de neurones est compos√© de 3 √©l√©ments :
1. **Neurone** ‚Äî L'unit√© de calcul √©l√©mentaire
2. **Couche** ‚Äî Un groupe de neurones identiques
3. **R√©seau** ‚Äî Un empilement de couches

On va les d√©tailler un par un.

---

## 1Ô∏è‚É£ Le Neurone

### Qu'est-ce qu'un neurone ?

Un neurone artificiel est inspir√© du neurone biologique. Il prend plusieurs **entr√©es**, les combine, et produit une **sortie**.

### Sch√©ma d'un neurone
```
Entr√©es          Poids          Somme pond√©r√©e      Sortie
  x1  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí  w1  ‚îÄ‚îê
  x2  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí  w2  ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚Üí Œ£(wi¬∑xi) + b ‚îÄ‚îÄ‚îÄ‚Üí activation ‚îÄ‚îÄ‚îÄ‚Üí y
  x3  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí  w3  ‚îÄ‚îò
                    +
                  biais (b)
```

### Calcul math√©matique

Un neurone effectue **2 op√©rations** :

#### 1. Combinaison lin√©aire (produit scalaire)
```
z = w1¬∑x1 + w2¬∑x2 + w3¬∑x3 + b
```

Ou en notation math√©matique :
```
z = Œ£(wi ¬∑ xi) + b
```

#### 2. Fonction d'activation
```
y = f(z)
```

O√π `f` peut √™tre : identit√©, seuil, sigmo√Øde, ReLU, etc.

### Exemple concret

Soit un neurone avec :
- Poids : `w = [0.5, -0.3, 0.2]`
- Biais : `b = 0.1`
- Entr√©es : `x = [1, 2, 3]`

**Calcul :**
```python
# √âtape 1 : Combinaison lin√©aire
z = 0.5*1 + (-0.3)*2 + 0.2*3 + 0.1
z = 0.5 - 0.6 + 0.6 + 0.1
z = 0.6

# √âtape 2 : Activation (par exemple ReLU)
y = max(0, z) = max(0, 0.6) = 0.6
```

### En Python (simplifi√©)
```python
def neuron(x, w, b, activation):
    # Produit scalaire
    z = sum([x[i] * w[i] for i in range(len(x))]) + b
    
    # Activation
    y = activation(z)
    
    return y
```

---

## 2Ô∏è‚É£ La Couche (Layer)

### Qu'est-ce qu'une couche ?

Une couche est un **groupe de neurones** qui :
- Re√ßoivent les **m√™mes entr√©es**
- Ont chacun leurs **propres poids et biais**
- Produisent chacun une **sortie**

### Sch√©ma d'une couche
```
Entr√©e (3 valeurs)       Couche (4 neurones)     Sortie (4 valeurs)

    x1  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí  [neurone 1]  ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí  y1
    x2  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí  [neurone 2]  ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí  y2
    x3  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí  [neurone 3]  ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí  y3
                        [neurone 4]  ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí  y4
```

### Exemple concret

Couche avec **2 neurones**, entr√©e de taille **3** :

**Neurone 1 :**
- Poids : `[0.5, 0.2, -0.1]`
- Biais : `0.0`

**Neurone 2 :**
- Poids : `[-0.3, 0.4, 0.6]`
- Biais : `0.2`

**Entr√©e :** `[1.0, 2.0, 0.5]`

**Calcul :**
```python
# Neurone 1
y1 = 0.5*1.0 + 0.2*2.0 + (-0.1)*0.5 + 0.0 = 0.85

# Neurone 2
y2 = (-0.3)*1.0 + 0.4*2.0 + 0.6*0.5 + 0.2 = 1.00

# Sortie de la couche
output = [0.85, 1.00]
```

### R√©seau totalement reli√© (fully-connected)

Dans ce projet, chaque neurone d'une couche est **connect√© √† TOUTES les sorties** de la couche pr√©c√©dente.

```
Couche 1 (3 neurones)    Couche 2 (2 neurones)

    [n1] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí [n1]
    [n2] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí [n2]
    [n3] ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 3Ô∏è‚É£ Le R√©seau (Network)

### Qu'est-ce qu'un r√©seau ?

Un r√©seau de neurones est une **composition de couches** :
```
Entr√©e ‚Üí Couche 1 ‚Üí Couche 2 ‚Üí ... ‚Üí Couche N ‚Üí Sortie
```

Chaque couche transforme le vecteur qu'elle re√ßoit et le passe √† la suivante.

### Architecture d'exemple
```
Input: [x1, x2, x3]  (3 valeurs)
   ‚Üì
Layer 1: 5 neurones  ‚Üí [y1, y2, y3, y4, y5]
   ‚Üì
Layer 2: 3 neurones  ‚Üí [z1, z2, z3]
   ‚Üì
Output: [z1, z2, z3]  (3 valeurs)
```

### Forward Pass (propagation avant)

C'est le processus qui fait circuler les donn√©es de l'entr√©e √† la sortie.

**Algorithme :**
```
1. Prendre le vecteur d'entr√©e
2. Pour chaque couche :
   a. Appliquer tous les neurones de la couche
   b. Obtenir un nouveau vecteur
3. Retourner la sortie de la derni√®re couche
```

**Exemple en pseudo-code :**
```python
def forward(input):
    current = input
    for layer in layers:
        current = layer.forward(current)
    return current
```

---

## üé® Fonctions d'activation

Une fonction d'activation transforme la sortie d'un neurone. Elle introduit de la **non-lin√©arit√©** dans le r√©seau.

### 1. Identit√©
```
f(x) = x
```
**Graphe :** Ligne droite √† 45¬∞  
**Usage :** Couche de sortie pour r√©gression

```
  y
  |     /
  |    /
  |   /
  |  /
  |‚îÄ/‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ x
```

**Code Python :**
```python
def identity(x):
    return x
```

---

### 2. Seuil (Step)
```
f(x) = 1 si x >= 0
       0 si x < 0
```
**Graphe :** Escalier  
**Usage :** Classification binaire (ancien, peu utilis√©)

```
  y
  |  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
  |  ‚îÇ
  |  ‚îÇ
  |‚îÄ‚îÄ‚îò
  |‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ x
```

**Code Python :**
```python
def step(x):
    return 1 if x >= 0 else 0
```

---

### 3. Sigmo√Øde
```
f(x) = 1 / (1 + e^(-x))
```
**Graphe :** Courbe en S  
**Usage :** Couche de sortie pour probabilit√©s (0 √† 1)

```
  y
1 |      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ
  |    ‚ï±
  |  ‚ï±
  |‚ï±
0 |‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ x
```

**Code Python :**
```python
import math

def sigmoid(x):
    return 1 / (1 + math.exp(-x))
```

**Propri√©t√©s :**
- Sortie entre 0 et 1
- Smooth (d√©rivable partout)
- Probl√®me : "vanishing gradient" pour x tr√®s grand/petit

---

### 4. ReLU (Rectified Linear Unit)
```
f(x) = max(0, x)
```
**Graphe :** Ligne bris√©e  
**Usage :** Couches cach√©es (le plus populaire)

```
  y
  |      ‚ï±
  |     ‚ï±
  |    ‚ï±
  |   ‚ï±
  |‚îÄ‚îÄ‚îò‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ x
```

**Code Python :**
```python
def relu(x):
    return max(0, x)
```

**Propri√©t√©s :**
- Simple et efficace
- Pas de vanishing gradient pour x > 0
- Probl√®me : neurones "morts" si x toujours < 0

---

## üìê Formules importantes

### Produit scalaire (dot product)
```
dot(x, w) = Œ£(xi ¬∑ wi)
          = x1¬∑w1 + x2¬∑w2 + ... + xn¬∑wn
```

**Exemple :**
```python
x = [1, 2, 3]
w = [0.5, -0.2, 0.1]

dot = 1*0.5 + 2*(-0.2) + 3*0.1 = 0.2
```

### Sortie d'un neurone
```
y = activation( Œ£(wi ¬∑ xi) + b )
```

### Sortie d'une couche
```
output = [neuron1(input), neuron2(input), ..., neuronN(input)]
```

---

## üß™ Exemple complet : R√©seau 2 couches

### Architecture
- **Entr√©e :** 2 valeurs
- **Couche 1 :** 3 neurones, activation ReLU
- **Couche 2 :** 1 neurone, activation identit√©

### Forward pass avec des valeurs

**Entr√©e :** `[1.0, 0.5]`

**Couche 1 (3 neurones) :**
```
Neurone 1: w=[0.5, 0.2], b=0.1  ‚Üí z=0.7  ‚Üí ReLU(0.7)=0.7
Neurone 2: w=[-0.3, 0.4], b=0.0 ‚Üí z=-0.1 ‚Üí ReLU(-0.1)=0.0
Neurone 3: w=[0.1, 0.6], b=0.2  ‚Üí z=0.6  ‚Üí ReLU(0.6)=0.6

Sortie couche 1: [0.7, 0.0, 0.6]
```

**Couche 2 (1 neurone) :**
```
Neurone 1: w=[0.5, -0.2, 0.3], b=0.0
z = 0.5*0.7 + (-0.2)*0.0 + 0.3*0.6 + 0.0 = 0.53
y = identity(0.53) = 0.53

Sortie finale: [0.53]
```

**R√©sultat :** Le r√©seau transforme `[1.0, 0.5]` en `[0.53]`

---

## ‚úÖ Points cl√©s √† retenir

1. **Neurone** = combinaison lin√©aire + activation
2. **Couche** = plusieurs neurones en parall√®le
3. **R√©seau** = empilement de couches
4. **Forward pass** = propagation de l'entr√©e √† la sortie
5. **Activation** = fonction non-lin√©aire (identit√©, seuil, sigmo√Øde, ReLU)

---

## üìö Ressources pour aller plus loin

- [Vid√©o 3Blue1Brown - Gradient descent](https://www.youtube.com/watch?v=IHZwWFHWa-w)
- [Article Wikip√©dia - R√©seau de neurones](https://fr.wikipedia.org/wiki/R%C3%A9seau_de_neurones_artificiels)
- [Playground TensorFlow](https://playground.tensorflow.org/) (visualiser un r√©seau)

---

**Prochaine √©tape :** [03 - Pr√©paration](03_preparation.md)
